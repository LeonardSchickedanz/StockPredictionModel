{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOkpzXVtaeILGyZxaVaGog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardSchickedanz/StockPredictionModel/blob/master/StockPredictionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Welcome to the Stock Price Prediction Model Repository!**\n",
        "\n",
        "This Google Colab repository is designed to help you predict stock prices using a combination of time series data, quarterly financial reports, and macroeconomic indicators. The project utilizes a **Long Short-Term Memory (LSTM)** neural network to forecast the stock price of a single stock with improved accuracy.\n",
        "\n",
        "## **What You Can Do Here**\n",
        "\n",
        "### **Optimize Hyperparameters**\n",
        "Use **Optuna** to automatically find the best hyperparameters for the LSTM model, ensuring optimal performance.\n",
        "\n",
        "### **Train the Model**\n",
        "Train the LSTM model using **Apple's stock data (AAPL)**. The model can incorporate various features, including historical stock prices, quarterly financial reports, and macroeconomic indicators.\n",
        "\n",
        "### **Test the Model**\n",
        "Evaluate the trained model on **Netflix's stock data (NFLX)** to assess how well it generalizes to unseen data.\n",
        "\n",
        "## **Data Sources**\n",
        "The data used in this project comes from two sources:\n",
        "- **Yahoo Finance (`yfinance`)** for historical stock price data.\n",
        "- **Alpha Vantage (`alpha_vantage`)** for fundamental company data through quarterly reports and macroeconomic indicators of the USA.\n",
        "  This data is retrieved using this [repository](https://github.com/RomelTorres/alpha_vantage) by *RomelTorres*.\n",
        "\n",
        "## **Getting Started**\n",
        "Simply follow the instructions above the code cells in order until you reach the **Model Usage Guide** at the end, where you can interact with the trained model and apply it to your own predictions.\n"
      ],
      "metadata": {
        "id": "tk5vFy76t_vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary packages and make imports.\n"
      ],
      "metadata": {
        "id": "OMTO_auakTDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy alpha_vantage pandas requests yfinance scikit-learn scipy optuna torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import yfinance as yf\n",
        "from google.colab import drive\n",
        "from alpha_vantage.fundamentaldata import FundamentalData\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import json\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "bDhDdbFX93ZZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Set Up Your Environment**\n",
        "Ensure you have a GPU enabled in your Google Colab runtime for faster computation. You can do this by going to **Runtime -> Change runtime type -> Hardware accelerator** and selecting **GPU**."
      ],
      "metadata": {
        "id": "IaJn2tqM1Tz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('GPU is available!')\n",
        "    print('GPU Device:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using CPU instead.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "locLzyhfDyN4",
        "outputId": "8a93e68c-c463-4752-cfdd-f06ba9bbdbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Using AlphaVantage Data**\n",
        "If you want to fetch the data yourself, you need to obtain an API key from [AlphaVantage](https://www.alphavantage.co/support/#api-key) and assign it to `API_KEY=`. The retrieved data will be stored in the temporary files of this Google Colab session.\n",
        "\n",
        "In this code cell, the data is fetched, cleaned, and then stored in the `data` folder. If you skip this step, the next code cell will automatically download the data from GitHub instead.\n",
        "\n"
      ],
      "metadata": {
        "id": "eeMinkzqfY11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = ''"
      ],
      "metadata": {
        "id": "bL2x1524RJ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0M9U8a4sxTd"
      },
      "outputs": [],
      "source": [
        "PROJECT_URL = '/content/PredictStockPrice'\n",
        "\n",
        "directory_economic_indicators_raw = f'{PROJECT_URL}/data/economic_indicators/raw'\n",
        "directory_economic_indicators_processed = f'{PROJECT_URL}/data/economic_indicators/processed'\n",
        "directory_tickers = f'{PROJECT_URL}/data/tickers'\n",
        "\n",
        "for directory in [directory_economic_indicators_raw, directory_economic_indicators_processed, f'{directory_tickers}/AAPL', f'{directory_tickers}/NFLX']:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "def api_raw_data_to_excel(ticker):\n",
        "    api_d_fundamental_data = FundamentalData(key=API_KEY, output_format='pandas')\n",
        "    d_quarterly_income, _ = api_d_fundamental_data.get_income_statement_quarterly(symbol=ticker)\n",
        "\n",
        "    d_time_series = yf.download(ticker, start=\"2000-01-01\", end=\"2024-12-17\", interval=\"1d\", auto_adjust=True)\n",
        "\n",
        "    d_time_series.index = d_time_series.index.tz_localize(None) # remove timezone\n",
        "\n",
        "    d_quarterly_income.to_excel(f'{directory_tickers}/{ticker}/d_quarterly_income_raw.xlsx', index=True)\n",
        "    d_time_series.to_excel(f'{directory_tickers}/{ticker}/d_timeseries_raw.xlsx', index=True)\n",
        "\n",
        "economic_indicators = (\n",
        "    'd_real_gdp',\n",
        "    'd_real_gdp_per_capita',\n",
        "    'd_federal_funds_rate',\n",
        "    'd_cpi',\n",
        "    'd_inflation',\n",
        "    'd_retail_sales',\n",
        "    'd_durables',\n",
        "    'd_unemployment',\n",
        "    'd_nonfarm_payroll'\n",
        ")\n",
        "\n",
        "def economic_indicators_to_excel():\n",
        "    main_url = 'https://www.alphavantage.co/query?function='\n",
        "    api_key_url = f'&apikey={API_KEY}'\n",
        "\n",
        "    url_list = (\n",
        "        'REAL_GDP&interval=annual',\n",
        "        'REAL_GDP_PER_CAPITA',\n",
        "        'FEDERAL_FUNDS_RATE&interval=monthly',\n",
        "        'CPI&interval=monthly',\n",
        "        'INFLATION',\n",
        "        'RETAIL_SALES',\n",
        "        'DURABLES',\n",
        "        'UNEMPLOYMENT',\n",
        "        'NONFARM_PAYROLL'\n",
        "    )\n",
        "\n",
        "    dataframes = []\n",
        "    for idx1, u in enumerate(url_list):\n",
        "        try:\n",
        "            url = f'{main_url}{u}{api_key_url}'\n",
        "            r = requests.get(url)\n",
        "            data = r.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data for {u}: {e}\")\n",
        "            return\n",
        "\n",
        "        if 'Information' in data and 'rate limit' in data['Information'].lower():\n",
        "            print(\"OUT OF API REQUESTS\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"API request successful\")\n",
        "\n",
        "        dates = []\n",
        "        values = []\n",
        "\n",
        "        for idx2 in range(len(data['data'])):\n",
        "            dates.append(data['data'][idx2]['date'])\n",
        "            values.append(data['data'][idx2]['value'])\n",
        "\n",
        "        assert len(dates) == len(values)\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"date\": dates,\n",
        "            \"value\": values\n",
        "        })\n",
        "        dataframes.append(df)\n",
        "\n",
        "        df.to_excel(f'{directory_economic_indicators_raw}/{economic_indicators[idx1]}_raw.xlsx', index=True)\n",
        "\n",
        "def stretch_data(data, min_date, max_date, date_column='date'):\n",
        "    data.set_index(date_column, inplace=True)\n",
        "    data = data.resample('D').ffill()\n",
        "\n",
        "    date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "    data = data.reindex(date_range)\n",
        "    data = data.reset_index(names=['date'])\n",
        "\n",
        "    data = data.iloc[::-1].reset_index(drop=True)\n",
        "    return data\n",
        "\n",
        "def date_to_unix_time_stamp(data, date_column='date'):\n",
        "    data[date_column] = pd.to_datetime(data[date_column])\n",
        "    data[date_column] = (data[date_column] - pd.Timestamp('1970-01-01')).dt.total_seconds()\n",
        "    return data\n",
        "\n",
        "def data_to_excel_main(ticker, call_economic_indicators_to_excel=False):\n",
        "    if call_economic_indicators_to_excel:\n",
        "        economic_indicators_to_excel()\n",
        "    api_raw_data_to_excel(ticker)\n",
        "\n",
        "    data_dict = {\n",
        "        \"real_gdp\": pd.read_excel(f'{directory_economic_indicators_raw}/d_real_gdp_raw.xlsx', index_col=0),\n",
        "        \"real_gdp_per_capita\": pd.read_excel(f'{directory_economic_indicators_raw}/d_real_gdp_per_capita_raw.xlsx', index_col=0),\n",
        "        \"federal_funds_rate\": pd.read_excel(f'{directory_economic_indicators_raw}/d_federal_funds_rate_raw.xlsx', index_col=0),\n",
        "        \"cpi\": pd.read_excel(f'{directory_economic_indicators_raw}/d_cpi_raw.xlsx', index_col=0),\n",
        "        \"inflation\": pd.read_excel(f'{directory_economic_indicators_raw}/d_inflation_raw.xlsx', index_col=0),\n",
        "        \"retail_sales\": pd.read_excel(f'{directory_economic_indicators_raw}/d_retail_sales_raw.xlsx', index_col=0),\n",
        "        \"durables\": pd.read_excel(f'{directory_economic_indicators_raw}/d_durables_raw.xlsx', index_col=0),\n",
        "        \"unemployment\": pd.read_excel(f'{directory_economic_indicators_raw}/d_unemployment_raw.xlsx', index_col=0),\n",
        "        \"nonfarm_payroll\": pd.read_excel(f'{directory_economic_indicators_raw}/d_nonfarm_payroll_raw.xlsx', index_col=0),\n",
        "        \"quarterly_income\": pd.read_excel(f'{directory_tickers}/{ticker}/d_quarterly_income_raw.xlsx', index_col=0),\n",
        "        \"time_series\": pd.read_excel(f'{directory_tickers}/{ticker}/d_timeseries_raw.xlsx', index_col=0)\n",
        "    }\n",
        "\n",
        "    # CLEAN DATA\n",
        "    d_quarterly_income = data_dict[\"quarterly_income\"]\n",
        "    d_time_series = data_dict[\"time_series\"]\n",
        "\n",
        "    d_time_series = d_time_series.iloc[2:].reset_index(drop=False)\n",
        "    d_time_series.columns = ['date', 'close', 'high', 'low', 'open', 'volume']\n",
        "    d_time_series = d_time_series[::-1].reset_index(drop=True)\n",
        "\n",
        "    d_quarterly_income.drop(columns=['depreciation', 'reportedCurrency'], errors='ignore', inplace=True)\n",
        "    d_quarterly_income.replace(\"None\", np.nan, inplace=True)\n",
        "    d_quarterly_income.fillna(0, inplace=True)\n",
        "    d_quarterly_income.rename(columns={'fiscalDateEnding': 'date'}, inplace=True)\n",
        "\n",
        "    data_dict[\"quarterly_income\"] = d_quarterly_income\n",
        "    data_dict[\"time_series\"] = d_time_series\n",
        "\n",
        "    for key, df in data_dict.items():\n",
        "        if key not in [\"quarterly_income\", \"time_series\"]:\n",
        "            new_column_name = key.lstrip('d_')\n",
        "            df.rename(columns={'value': new_column_name}, inplace=True)\n",
        "\n",
        "    for key, df in data_dict.items():\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    max_d = min(df['date'].max() for df in data_dict.values())\n",
        "    min_d = max(df['date'].min() for df in data_dict.values())\n",
        "\n",
        "    for key, df in data_dict.items():\n",
        "        data_dict[key] = stretch_data(data=df, min_date=min_d, max_date=max_d)\n",
        "        data_dict[key] = date_to_unix_time_stamp(data_dict[key])\n",
        "\n",
        "    for key, df in data_dict.items():\n",
        "        df.to_excel(f'{directory_economic_indicators_processed}/d_{key}.xlsx')\n",
        "\n",
        "    data_dict[\"time_series\"].to_excel(f'{directory_tickers}/{ticker}/d_timeseries.xlsx')\n",
        "    data_dict[\"quarterly_income\"].to_excel(f'{directory_tickers}/{ticker}/d_quarterly_income.xlsx')\n",
        "\n",
        "if API_KEY != '':\n",
        "    data_to_excel_main('AAPL', True)\n",
        "    data_to_excel_main('NFLX', False)\n",
        "    print(\"All folders and files have been successfully loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading Model Files from GitHub**\n",
        "Run the next cell to download pre-trained model files and hyperparameters from GitHub. This works whether or not you fetched the data manually in the previous step. If you skipped the data retrieval step, the GitHub version of the data will be used automatically, ensuring that the necessary files are available for training and testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "nyFkaBcF7fV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_USER = \"LeonardSchickedanz\"\n",
        "GITHUB_REPO = \"StockPredictionModel\"\n",
        "BRANCH = \"master\"\n",
        "\n",
        "PROJECT_URL = \"/content/PredictStockPrice\"\n",
        "os.makedirs(PROJECT_URL, exist_ok=True)\n",
        "\n",
        "FOLDERS = [\"model\"]\n",
        "\n",
        "GITHUB_API_URL = f\"https://api.github.com/repos/{GITHUB_USER}/{GITHUB_REPO}/contents\"\n",
        "\n",
        "def download_github_folder(folder_name, local_folder):\n",
        "    folder_url = f\"{GITHUB_API_URL}/{folder_name}?ref={BRANCH}\"\n",
        "    response = requests.get(folder_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        files = response.json()\n",
        "        os.makedirs(local_folder, exist_ok=True)\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(local_folder, file[\"name\"])\n",
        "\n",
        "            if file[\"type\"] == \"file\":\n",
        "                file_url = file[\"download_url\"]\n",
        "                os.system(f\"wget -q -O {file_path} {file_url}\")\n",
        "            elif file[\"type\"] == \"dir\":\n",
        "                download_github_folder(f\"{folder_name}/{file['name']}\", file_path)\n",
        "    else:\n",
        "        print(f\"error fetching {folder_name}: {response.status_code}\")\n",
        "\n",
        "if not os.path.exists(os.path.join(PROJECT_URL, \"data\")):\n",
        "    FOLDERS.append(\"data\")\n",
        "\n",
        "for folder in FOLDERS:\n",
        "    download_github_folder(folder, os.path.join(PROJECT_URL, folder))\n",
        "\n",
        "print(\"All folders and files have been successfully loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyitZ77KAMZL",
        "outputId": "8d860adb-6702-41dd-91eb-182d07ce510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All folders and files have been successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading and Processing Data**\n",
        "In this step, previously stored data is loaded from Excel files, merged, and converted into a tensor for model training and evaluation.\n",
        "\n",
        "- **Time series data** and additional economic indicators are loaded from the `data` folder.\n",
        "- **Macroeconomic indicators** such as GDP, inflation, unemployment, and retail sales are included if enabled.\n",
        "- **Quarterly income data** is loaded for the specified stock ticker if selected.\n",
        "\n",
        "Furthermore the methods for plotting the results are defined.\n",
        "\n"
      ],
      "metadata": {
        "id": "37R0w4vcPDsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t_ = tensor\n",
        "# d_ = raw data\n",
        "# f_ = features\n",
        "\n",
        "directory_economic_indicators_raw = f'{PROJECT_URL}/data/economic_indicators/raw'\n",
        "directory_economic_indicators_processed = f'{PROJECT_URL}/data/economic_indicators/processed'\n",
        "directory_tickers = f'{PROJECT_URL}/data/tickers'\n",
        "\n",
        "def merge_dataframes(dataframes):\n",
        "    d_merged = pd.concat(dataframes, axis=1)\n",
        "    d_merged = d_merged.loc[:, ~d_merged.columns.duplicated()]\n",
        "    if 'value' in d_merged.columns:\n",
        "        d_merged = d_merged.drop(columns='value')\n",
        "\n",
        "    return d_merged\n",
        "\n",
        "def data_main(ticker, with_quarterly_income=True, with_econ=True, call_data_to_excel_main=False):\n",
        "    data_dict = {\n",
        "        \"time_series\": pd.read_excel(f'{directory_tickers}/{ticker}/d_timeseries.xlsx', index_col=0),\n",
        "        \"econ_real_gdp\": pd.read_excel(f'{directory_economic_indicators_processed}/d_real_gdp.xlsx', index_col=0),\n",
        "        \"econ_real_gdp_per_capita\": pd.read_excel(f'{directory_economic_indicators_processed}/d_real_gdp_per_capita.xlsx', index_col=0),\n",
        "        \"econ_federal_funds_rate\": pd.read_excel(f'{directory_economic_indicators_processed}/d_federal_funds_rate.xlsx', index_col=0),\n",
        "        \"econ_cpi\": pd.read_excel(f'{directory_economic_indicators_processed}/d_cpi.xlsx', index_col=0),\n",
        "        \"econ_inflation\": pd.read_excel(f'{directory_economic_indicators_processed}/d_inflation.xlsx', index_col=0),\n",
        "        \"econ_retail_sales\": pd.read_excel(f'{directory_economic_indicators_processed}/d_retail_sales.xlsx', index_col=0),\n",
        "        \"econ_durables\": pd.read_excel(f'{directory_economic_indicators_processed}/d_durables.xlsx', index_col=0),\n",
        "        \"econ_unemployment\": pd.read_excel(f'{directory_economic_indicators_processed}/d_unemployment.xlsx', index_col=0),\n",
        "        \"econ_nonfarm_payroll\": pd.read_excel(f'{directory_economic_indicators_processed}/d_nonfarm_payroll.xlsx', index_col=0),\n",
        "        \"quarterly_income\": pd.read_excel(f'{directory_tickers}/{ticker}/d_quarterly_income.xlsx', index_col=0)\n",
        "    }\n",
        "\n",
        "    if call_data_to_excel_main:\n",
        "        data_to_excel_main(ticker)\n",
        "\n",
        "    dataframes = [data_dict[\"time_series\"]]\n",
        "\n",
        "    if with_econ:\n",
        "        dataframes.extend(value for key, value in data_dict.items() if key.startswith(\"econ_\"))\n",
        "    if with_quarterly_income and \"quarterly_income\" in data_dict:\n",
        "        dataframes.append(data_dict[\"quarterly_income\"])\n",
        "\n",
        "    d_combined = merge_dataframes(dataframes)\n",
        "\n",
        "    d_combined.to_excel(f'{PROJECT_URL}/data/d_combined.xlsx') # for debugging\n",
        "    t_combined = torch.tensor(d_combined.values).float()\n",
        "    return t_combined\n",
        "\n",
        "def plot(losses, test_losses, prediction, y_test, ticker, t_combined, look_back_days, forecast_horizon, alpha=0.2, with_baselines=True):\n",
        "    plot_losses(losses, test_losses)\n",
        "\n",
        "    d_time_series = pd.read_excel(f'{PROJECT_URL}/data/tickers/{ticker}/d_timeseries.xlsx', index_col=0)\n",
        "    date = d_time_series['date'].apply(lambda x: datetime.fromtimestamp(x).date())\n",
        "    date = date[:len(prediction)]\n",
        "    date = date[-len(y_test):]\n",
        "\n",
        "    prediction = prediction[::-1]\n",
        "    y_test = y_test[::-1]\n",
        "\n",
        "    prices = t_combined[:, 1].cpu().numpy()\n",
        "    baselines = {}\n",
        "\n",
        "    avg_price_predictions = np.full(len(y_test), np.nan)\n",
        "    naive_predictions = np.full(len(y_test), np.nan)\n",
        "    linear_reg_predictions = np.full(len(y_test), np.nan)\n",
        "    ema_predictions = np.full(len(y_test), np.nan)\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if i + look_back_days + forecast_horizon <= len(prices):\n",
        "            avg_price_predictions[i] = np.mean(prices[i:i + look_back_days])\n",
        "            naive_predictions[i] = prices[i + look_back_days - 1]\n",
        "\n",
        "            x_train = np.arange(look_back_days).reshape(-1, 1)\n",
        "            y_train = prices[i:i + look_back_days].astype(float)\n",
        "            model = LinearRegression().fit(x_train, y_train)\n",
        "            linear_reg_predictions[i] = model.predict(np.array([[look_back_days]]))[0]\n",
        "\n",
        "            ema = prices[i]\n",
        "            for j in range(1, look_back_days):\n",
        "                ema = alpha * prices[i + j] + (1 - alpha) * ema\n",
        "            ema_predictions[i] = ema\n",
        "\n",
        "    baselines['average_price'] = avg_price_predictions\n",
        "    baselines['naive_persistence'] = naive_predictions\n",
        "    baselines['linear_regression'] = linear_reg_predictions\n",
        "    baselines['exponential_moving_average'] = ema_predictions\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=date, y=prediction, mode='lines', name='Prediction', line=dict(color='red')))\n",
        "    fig.add_trace(go.Scatter(x=date, y=y_test, mode='lines', name='Actual Price', line=dict(color='blue')))\n",
        "\n",
        "    if with_baselines:\n",
        "        fig.add_trace(go.Scatter(x=date, y=baselines['average_price'], mode='lines', name='Avg Price Baseline', line=dict(color='green', dash='dash')))\n",
        "        fig.add_trace(go.Scatter(x=date, y=baselines['naive_persistence'], mode='lines', name='Naive Persistence', line=dict(color='orange', dash='dash')))\n",
        "        fig.add_trace(go.Scatter(x=date, y=baselines['linear_regression'], mode='lines', name='Linear Regression', line=dict(color='purple', dash='dash')))\n",
        "        fig.add_trace(go.Scatter(x=date, y=baselines['exponential_moving_average'], mode='lines', name='Exponential Moving Avg', line=dict(color='brown', dash='dash')))\n",
        "\n",
        "    fig.update_layout(title=f'Stock Prediction for {ticker}', xaxis_title='Date', yaxis_title='Price', plot_bgcolor='white')\n",
        "    fig.show()\n",
        "\n",
        "def plot_losses(losses, test_losses):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=losses,\n",
        "        mode='lines',\n",
        "        name='Training Loss',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=test_losses,\n",
        "        mode='lines',\n",
        "        name='Test Loss',\n",
        "        line=dict(color='orange')\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Training and Test Loss Over Time',\n",
        "        xaxis_title='Epoch',\n",
        "        yaxis_title='Loss',\n",
        "        legend=dict(x=0.85, y=0.95),\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
        "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "mlRSMIG5wrYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run to define the core methods for training, optimization and testing."
      ],
      "metadata": {
        "id": "HY9Oijx-7jYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "def prepare_training_data(tensor, look_back_days=90, forecast_horizon=30, closed_price_column=1, with_scaler=True):\n",
        "    tensor = torch.flip(tensor, [0])\n",
        "\n",
        "    size_rows = tensor.size(0)\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    scalers = []\n",
        "    scaled_columns = []\n",
        "\n",
        "    price_column = tensor[:, closed_price_column].reshape(-1, 1).cpu()\n",
        "\n",
        "    if with_scaler:\n",
        "        price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        price_column_scaled = torch.tensor(\n",
        "            price_scaler.fit_transform(price_column.cpu().numpy()),\n",
        "            dtype=tensor.dtype\n",
        "        )\n",
        "    else:\n",
        "        price_scaler = None\n",
        "        price_column_scaled = price_column  # No scaling\n",
        "\n",
        "    for col_idx in range(tensor.size(1)):\n",
        "        column_data = tensor[:, col_idx].unsqueeze(1)\n",
        "\n",
        "        if with_scaler:\n",
        "            scaler = StandardScaler()\n",
        "            if col_idx == closed_price_column:\n",
        "                scaled_column = price_column_scaled\n",
        "            else:\n",
        "                scaler.fit(column_data.cpu().numpy())\n",
        "                scaled_column = torch.tensor(\n",
        "                    scaler.transform(column_data.cpu().numpy()),\n",
        "                    dtype=tensor.dtype\n",
        "                )\n",
        "                scalers.append(scaler)\n",
        "        else:\n",
        "            scaled_column = column_data  # No scaling\n",
        "\n",
        "        scaled_columns.append(scaled_column)\n",
        "\n",
        "    scaled_tensor = torch.cat(scaled_columns, dim=1)\n",
        "\n",
        "    # create sequences\n",
        "    for idx in range(size_rows - look_back_days - forecast_horizon):\n",
        "        x_block = scaled_tensor[idx:idx + look_back_days, :]\n",
        "        y_value = scaled_tensor[idx + look_back_days + forecast_horizon, closed_price_column]\n",
        "        x.append(x_block)\n",
        "        y.append(y_value)\n",
        "\n",
        "    x = torch.stack(x)\n",
        "    y = torch.stack(y)\n",
        "\n",
        "    # train test split\n",
        "    split_ratio = 0.8\n",
        "    split_index = int(len(x) * split_ratio)\n",
        "    x_train = x[:split_index]\n",
        "    y_train = y[:split_index]\n",
        "    x_test = x[split_index:]\n",
        "    y_test = y[split_index:]\n",
        "\n",
        "    # reshape\n",
        "    y_train = y_train.view(-1, 1)\n",
        "    y_test = y_test.view(-1, 1)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test, scalers if with_scaler else None, price_scaler if with_scaler else None\n",
        "\n",
        "def evaluate_prediction(actual, forecast):\n",
        "    if isinstance(actual, torch.Tensor):\n",
        "        actual = actual.numpy()\n",
        "    if isinstance(forecast, torch.Tensor):\n",
        "        forecast = forecast.numpy()\n",
        "\n",
        "    diff = actual - forecast\n",
        "\n",
        "    mae = np.mean(np.abs(diff))\n",
        "    mse = np.mean(diff ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs(diff / np.where(actual == 0, np.finfo(float).eps, actual))) * 100\n",
        "\n",
        "    r_squared = 1 - (np.sum((actual - forecast) ** 2) / np.sum((actual - np.mean(actual)) ** 2))\n",
        "\n",
        "    spearman_corr, _ = spearmanr(actual, forecast)\n",
        "    pearson_corr, _ = pearsonr(actual, forecast)\n",
        "\n",
        "    direction_true = np.sign(np.diff(actual))\n",
        "    direction_pred = np.sign(np.diff(forecast))\n",
        "    directional_accuracy = np.mean(direction_true == direction_pred)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(f'Mean absolute error: {mae:.2f}')\n",
        "    print(f'Mean squared error: {mse:.2f}')\n",
        "    print(f'Root mean squared error: {rmse:.2f}')\n",
        "    print(f'R-Squared: {r_squared:.2f}')\n",
        "    print(f'Mean absolute percentage error: {mape:.2f}%')\n",
        "    print(f\"Spearman's correlation: {spearman_corr:.2f}\")\n",
        "    print(f\"Pearson's correlation: {pearson_corr:.2f}\")\n",
        "    print(f\"Directional Accuracy: {directional_accuracy * 100:.2f}%\")\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size, dropout_rate):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.lstm_layers.append(nn.LSTM(input_size=input_size, hidden_size=hidden_layers[0], batch_first=True))\n",
        "\n",
        "        for i in range(1, len(hidden_layers)):\n",
        "            self.lstm_layers.append(\n",
        "                nn.LSTM(input_size=hidden_layers[i - 1], hidden_size=hidden_layers[i], batch_first=True))\n",
        "\n",
        "        self.fc = nn.Linear(hidden_layers[-1], output_size)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for lstm in self.lstm_layers:\n",
        "            x, _ = lstm(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def train_and_test(x_train, x_test, y_train, y_test, scalers, price_scaler, ticker, model, optimizer, criterion, model_name, t_combined, look_back_days, forecast_horizon, epochs=200):\n",
        "    start_time = time.time()\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 20\n",
        "    no_improve = 0\n",
        "    best_prediction = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_pred = model(batch_x)\n",
        "            loss = criterion(batch_pred, batch_y)\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            device = next(model.parameters()).device\n",
        "            x_test = x_test.to(device)\n",
        "            y_pred_test = model(x_test)\n",
        "\n",
        "            test_loss = criterion(y_pred_test, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                best_prediction = y_pred_test.cpu().detach().numpy().flatten()\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if no_improve >= patience:\n",
        "                print(f\"EARLY STOPPING AT EPOCH {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {avg_loss}, Test Loss = {test_loss.item()}\")\n",
        "\n",
        "    final_prediction = price_scaler.inverse_transform(best_prediction.reshape(-1, 1)).flatten()\n",
        "    y_test_cpu = y_test.cpu().numpy()\n",
        "    y_test_descaled = price_scaler.inverse_transform(y_test_cpu.reshape(-1, 1)).flatten()\n",
        "\n",
        "    torch.save(model.state_dict(), f'{PROJECT_URL}/model/weights/{model_name}.pth')\n",
        "\n",
        "    np_y_pred = np.array(final_prediction)\n",
        "    df = pd.DataFrame(np_y_pred)\n",
        "    os.makedirs(f\"{PROJECT_URL}/model/output/{model_name}\", exist_ok=True)\n",
        "    df.to_csv(f\"{PROJECT_URL}/model/output/{model_name}/prediction.csv\", index=False)\n",
        "\n",
        "    pd.DataFrame(losses).to_csv(f\"{PROJECT_URL}/model/output/{model_name}/losses.csv\", index=False)\n",
        "    pd.DataFrame(test_losses).to_csv(f\"{PROJECT_URL}/model/output/{model_name}/test_losses.csv\", index=False)\n",
        "\n",
        "    evaluate_prediction(y_test_descaled, final_prediction)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"The duration was {(end_time - start_time) // 60} minutes and {(end_time - start_time) % 60} seconds.\")\n",
        "    plot(losses, test_losses, final_prediction, y_test_descaled, ticker, t_combined, look_back_days, forecast_horizon)\n",
        "    plot(losses, test_losses, final_prediction, y_test_descaled, ticker, t_combined, look_back_days, forecast_horizon, with_baselines=False)\n",
        "\n",
        "def test_once(x_train, x_test, y_train, y_test, rest_scaler, price_scaler, test_ticker, model, optimizer, criterion, model_name, t_combined, look_back_days, forecast_horizon, call_data_to_excel_main=False):\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(x_test)\n",
        "\n",
        "    prediction_descaled = price_scaler.inverse_transform(y_pred_test.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "    y_test_descaled = price_scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "\n",
        "    losses = pd.read_csv(f\"{PROJECT_URL}/model/output/{model_name}/losses.csv\")['0'].tolist()\n",
        "    test_losses = pd.read_csv(f\"{PROJECT_URL}/model/output/{model_name}/test_losses.csv\")['0'].tolist()\n",
        "\n",
        "    plot(losses, test_losses, prediction_descaled, y_test_descaled, test_ticker, t_combined, look_back_days, forecast_horizon)\n",
        "\n",
        "    evaluate_prediction(y_test_descaled, prediction_descaled)\n",
        "\n",
        "def optimize(model_name='all_features_scaled', with_quarterly_income=True, with_econ = True, with_scaler=True, trials=50):\n",
        "    start_time = time.time()\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")  # minimize loss\n",
        "    study.optimize(lambda trial: objective(trial=trial, with_quarterly_income=with_quarterly_income, with_econ=with_econ, with_scaler=with_scaler, model_name=model_name), n_trials=trials)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_loss = study.best_value\n",
        "\n",
        "    best_params['best_loss'] = best_loss\n",
        "    file_path = f'{PROJECT_URL}/model/hyperparameters/{model_name}.json'\n",
        "    os.makedirs(os.path.dirname(f'{PROJECT_URL}/model/hyperparameters'), exist_ok=True)\n",
        "\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(best_params, f, indent=4)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"The duration was {(end_time - start_time) // 60} minutes and {(end_time - start_time) % 60} seconds.\")\n",
        "\n",
        "def objective(trial, with_quarterly_income=True, with_econ=True, with_scaler=True, model_name='all_features_scaled'):\n",
        "\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
        "    hidden_layers = [trial.suggest_int(f\"hidden_size_{i}\", 50, 1000) for i in range(num_layers)]\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-2, log=True)\n",
        "    look_back_days = trial.suggest_int(\"look_back_days\", 20, 120)\n",
        "    forecast_horizon = trial.suggest_int(\"forecast_horizon\", 7, 60)\n",
        "\n",
        "    input_size = 6\n",
        "    if with_quarterly_income:\n",
        "        input_size += 23\n",
        "    if with_econ:\n",
        "        input_size += 9\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = LSTMModel(input_size=input_size, hidden_layers=hidden_layers, output_size=1, dropout_rate=dropout_rate)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    ticker = 'AAPL'\n",
        "    t_combined = data_main(ticker, with_quarterly_income, with_econ)\n",
        "    x_train, x_test, y_train, y_test, rest_scaler, price_scaler = prepare_training_data(\n",
        "        t_combined, with_scaler=with_scaler, look_back_days=look_back_days, forecast_horizon=forecast_horizon\n",
        "    )\n",
        "\n",
        "    x_train, x_test, y_train, y_test = x_train.to(device), x_test.to(device), y_train.to(device), y_test.to(device)\n",
        "\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "    train_and_test(x_train, x_test, y_train, y_test, rest_scaler, price_scaler, ticker, model, optimizer, criterion, model_name, t_combined, look_back_days, forecast_horizon, epochs=10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(x_test)\n",
        "        loss = criterion(y_pred_test, y_test)\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def train_and_test(x_train, x_test, y_train, y_test, scalers, price_scaler, ticker, model, optimizer, criterion, model_name,t_combined, look_back_days, forecast_horizon, epochs=200):\n",
        "    start_time = time.time()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    x_train, x_test, y_train, y_test = x_train.to(device), x_test.to(device), y_train.to(device), y_test.to(device)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 30\n",
        "    no_improve = 0\n",
        "    best_prediction = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_pred = model(batch_x)\n",
        "\n",
        "            loss = criterion(batch_pred, batch_y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred_test = model(x_test.to(device))\n",
        "            test_loss = criterion(y_pred_test, y_test)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                best_prediction = y_pred_test.cpu().detach().numpy().flatten()\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            if no_improve >= patience:\n",
        "                print(f\"EARLY STOPPING AT EPOCH {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {avg_loss}, Test Loss = {test_loss.item()}\")\n",
        "\n",
        "    if scalers is not None:\n",
        "        final_prediction = price_scaler.inverse_transform(best_prediction.reshape(-1, 1)).flatten()\n",
        "        y_test_descaled = price_scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "    else:\n",
        "        final_prediction = best_prediction\n",
        "        y_test_descaled = y_test.cpu().numpy().flatten()\n",
        "\n",
        "    print(f\"Saved parameters for model: {PROJECT_URL}/model/weights/{model_name}.pth\", )\n",
        "    torch.save(model.state_dict(), f'{PROJECT_URL}/model/weights/{model_name}.pth')\n",
        "\n",
        "    np_y_pred = np.array(final_prediction)\n",
        "    df = pd.DataFrame(np_y_pred)\n",
        "    os.makedirs(f\"{PROJECT_URL}/model/output/{model_name}\", exist_ok=True)\n",
        "    df.to_csv(f\"{PROJECT_URL}/model/output/{model_name}/prediction.csv\", index=False)\n",
        "\n",
        "    pd.DataFrame(losses).to_csv(f\"{PROJECT_URL}/model/output/{model_name}/losses.csv\", index=False)\n",
        "    pd.DataFrame(test_losses).to_csv(f\"{PROJECT_URL}/model/output/{model_name}/test_losses.csv\", index=False)\n",
        "\n",
        "    evaluate_prediction(y_test_descaled, final_prediction)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"The duration was {(end_time - start_time) // 60} minutes and {(end_time - start_time) % 60} seconds.\")\n",
        "    plot(losses, test_losses, final_prediction, y_test_descaled, ticker, t_combined, look_back_days, forecast_horizon)\n",
        "\n",
        "def choose_hyperparameters_manually(\n",
        "        model_name='all_features_scaled',\n",
        "        with_quarterly_income=True,\n",
        "        with_econ=True,\n",
        "        num_layers=2,\n",
        "        hidden_neurons_per_layer=None,\n",
        "        dropout_rate=0.2,\n",
        "        learning_rate=0.001,\n",
        "        look_back_days=60,\n",
        "        forecast_horizon=30\n",
        "):\n",
        "    input_size = 6  # timeseries features + date\n",
        "    if with_quarterly_income:\n",
        "        input_size += 23\n",
        "    if with_econ:\n",
        "        input_size += 9\n",
        "\n",
        "    if hidden_neurons_per_layer is None:\n",
        "        hidden_neurons_per_layer = [64] * num_layers\n",
        "\n",
        "    if len(hidden_neurons_per_layer) != num_layers:\n",
        "        raise ValueError(\n",
        "            f\"Expected {num_layers} hidden layers, but got {len(hidden_neurons_per_layer)} in hidden_neurons_per_layer\")\n",
        "\n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_layers=hidden_neurons_per_layer,\n",
        "        output_size=1,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "\n",
        "    return model, optimizer, criterion, look_back_days, forecast_horizon\n",
        "\n",
        "def load_hyperparameters(model_name='all_features_scaled', with_quarterly_income=True, with_econ=True):\n",
        "    file_path = f'{PROJECT_URL}/model/hyperparameters/{model_name}.json'\n",
        "    with open(file_path, 'r') as f:\n",
        "        best_params = json.load(f)\n",
        "\n",
        "    input_size = 6\n",
        "    if with_quarterly_income: input_size += 23\n",
        "    if with_econ: input_size += 9\n",
        "    hidden_layers = [best_params[f\"hidden_size_{i}\"] for i in range(best_params[\"num_layers\"])]\n",
        "    dropout_rate = best_params[\"dropout_rate\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "    look_back_days = best_params[\"look_back_days\"]\n",
        "    forecast_horizon = best_params[\"forecast_horizon\"]\n",
        "\n",
        "    model = LSTMModel(\n",
        "        input_size=input_size,\n",
        "        hidden_layers=hidden_layers,\n",
        "        output_size=1,\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "\n",
        "    return model, optimizer, criterion, look_back_days, forecast_horizon\n",
        "\n",
        "def main(action: str, model: str):\n",
        "    model_list = ('all_features_scaled', 'all_features_unscaled', 'timeseries_quarterly_reports_scaled', 'timeseries_econ_scaled')\n",
        "\n",
        "    match model:\n",
        "      case 'all_features_scaled': with_quarterly_income = True; with_econ = True; with_scaler = True;\n",
        "      case 'all_features_unscaled': with_quarterly_income = True; with_econ = True; with_scaler = False;\n",
        "      case 'timeseries_quarterly_reports_scaled': with_quarterly_income = True; with_econ = False; with_scaler = True;\n",
        "      case 'timeseries_econ_scaled': with_quarterly_income = False; with_econ = True; with_scaler = True;\n",
        "\n",
        "    match action:\n",
        "        case 'optimize':\n",
        "            optimize(model, with_quarterly_income = with_quarterly_income, with_econ = with_econ, with_scaler=with_scaler, trials=100)\n",
        "\n",
        "        case 'train':\n",
        "            ticker = 'AAPL'\n",
        "            t_combined = data_main(ticker, with_quarterly_income = with_quarterly_income, with_econ = with_econ)\n",
        "            model, optimizer, criterion, look_back_days, forecast_horizon = load_hyperparameters(model)\n",
        "            #model, optimizer, criterion, look_back_days, forecast_horizon = choose_hyperparameters_manually(with_econ=with_econ, with_quarterly_income=with_quarterly_income,num_layers=3, hidden_neurons_per_layer=[543, 831, 486], dropout_rate=0.3434091618324021, learning_rate=0.0010729338976430665, look_back_days = 117, forecast_horizon = 16)\n",
        "            x_train, x_test, y_train, y_test, scalers, price_scaler= prepare_training_data(t_combined, look_back_days, forecast_horizon)\n",
        "            train_and_test(x_train, x_test, y_train, y_test, scalers, price_scaler, ticker, model, optimizer, criterion, model, t_combined, look_back_days, forecast_horizon, epochs=2)\n",
        "\n",
        "        case 'test':\n",
        "            ticker = 'NFLX'\n",
        "            t_combined = data_main(ticker)\n",
        "            model, optimizer, criterion, look_back_days, forecast_horizon = load_hyperparameters(model)\n",
        "            print(\"model parameter: \", model)\n",
        "            model.load_state_dict(torch.load(f'{PROJECT_URL}/model/weights/{model}.pth'))\n",
        "\n",
        "            x_train, x_test, y_train, y_test, rest_scaler, price_scaler = prepare_training_data(t_combined, look_back_days, forecast_horizon)\n",
        "            test_once(x_train, x_test, y_train, y_test, rest_scaler, price_scaler, ticker, model, optimizer, criterion, model, t_combined, look_back_days, forecast_horizon)\n",
        "\n"
      ],
      "metadata": {
        "id": "nb16yG03ynBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1c6e6c-b859-4205-ccea-608447813560",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Usage Guide**\n",
        "\n",
        "## **Available Models**\n",
        "You may choose between four different models:\n",
        "\n",
        "- `all_features_scaled`: Uses time series data, quarterly reports, and macroeconomic data, all scaled.\n",
        "- `all_features_unscaled`: Same as above but without scaling.\n",
        "- `timeseries_quarterly_reports_scaled`: Uses time series and quarterly reports, both scaled.\n",
        "- `timeseries_econ_scaled`: Uses time series data and macroeconomic data, both scaled.\n",
        "\n",
        "## **Available Actions**\n",
        "You can perform three different actions:\n",
        "\n",
        "- `optimize`\n",
        "- `train`\n",
        "- `test`\n",
        "\n",
        "---\n",
        "\n",
        "## **Optimize**\n",
        "\n",
        "Hyperparameter optimization is done using Optuna with a default of 100 trials. The best hyperparameters are automatically saved as a `.json` file in `model/hyperparameters` under the respective model name. The default stock is Apple.\n",
        "\n",
        "### **Customization**\n",
        "- Adjust hyperparameter ranges in the `objective` method.\n",
        "- Modify the number of trials by setting the `trials` parameter in the `main` function (`case: 'optimize'`).\n",
        "\n",
        "---\n",
        "\n",
        "## **Train**\n",
        "\n",
        "Training a model involves loading optimal hyperparameters and training on Apple's stock (AAPL).\n",
        "\n",
        "### **Customization**\n",
        "- Load best hyperparameters automatically using `load_hyperparameters`.\n",
        "- Manually set hyperparameters with `choose_hyperparameters_manually`.\n",
        "-Adjust the number of epochs (default = 200) in the `train_and_test` method via the `epochs` parameter.\n",
        "\n",
        "---\n",
        "\n",
        "## **Test**\n",
        "\n",
        "Testing a trained model is performed on Netflix stock (NFLX).\n",
        "\n",
        "### **Process**\n",
        "- Loads the optimal hyperparameters automatically.\n",
        "- Loads the trained model weights automatically.\n",
        "- Runs the model once and displays results in the console.\n",
        "\n",
        "---\n",
        "\n",
        "By downloading the files from GitHub, hyperparameters and weights already exist. However, they will be overwritten as soon as you run `optimize` or `train`.\n"
      ],
      "metadata": {
        "id": "klf5TwM-9Lrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main(model='all_features_scaled', action='optimize')"
      ],
      "metadata": {
        "id": "pSJsHSli71XR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}